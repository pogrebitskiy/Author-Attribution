{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:53:08.711183Z",
     "start_time": "2024-04-04T06:53:05.226115Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39785d28fd982b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:53:08.886817Z",
     "start_time": "2024-04-04T06:53:08.713457Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the data pickle\n",
    "df = pandas.read_pickle('../data/cleaned_articles.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d0bace00d57f8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:59:34.001424Z",
     "start_time": "2024-04-04T06:53:08.888461Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14482\r"
     ]
    }
   ],
   "source": [
    "# Prepare the data for Doc2Vec\n",
    "\n",
    "# tokenize the text\n",
    "df['tokens'] = df['clean_content'].apply(word_tokenize)\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df['tokens'])]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "model = Doc2Vec(documents, vector_size=768, window=5, min_count=1, workers=12)\n",
    "\n",
    "# Define a function to vectorize a document\n",
    "def vectorize_doc(row):\n",
    "    print(f'{row.name}\\r', end='')\n",
    "    doc = row['tokens']\n",
    "    return model.infer_vector(doc)\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "df['doc2vec'] = df.apply(vectorize_doc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e630b92-ec7c-4e36-b136-050d2a9e78f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:59:34.015737Z",
     "start_time": "2024-04-04T06:59:34.004694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the authors  with label encoding\n",
    "author_encoder = LabelEncoder()\n",
    "authors = author_encoder.fit_transform(df['author'].values.ravel())\n",
    "with open('../pickles/author_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(author_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3283259f-9f75-42d5-bd31-1dee89fe0e20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:59:34.084340Z",
     "start_time": "2024-04-04T06:59:34.017267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save all the features to binarized numpy files\n",
    "with open('../data/doc2vec.npy', 'wb') as f:\n",
    "    array = np.stack(df['doc2vec'].values)\n",
    "    np.save(f, array)\n",
    "\n",
    "np.save('../data/authors_encoded.npy', authors)\n",
    "np.save('../data/authors.npy', df['author'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583a7122507e1eb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:59:34.089972Z",
     "start_time": "2024-04-04T06:59:34.086194Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 114/114 [08:47<00:00,  4.63s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Assuming your dataframe is named df and the content column is 'content'\n",
    "content_list = df['clean_content'].tolist()\n",
    "\n",
    "# Tokenize all content at once with a smaller batch size and sequence length\n",
    "batch_size = 128\n",
    "max_length = 512\n",
    "embeddings_list = []\n",
    "\n",
    "# Process data in batches\n",
    "for i in tqdm(range(0, len(content_list), batch_size), desc=\"Processing batches\"):\n",
    "    batch = content_list[i:i+batch_size]\n",
    "    tokens_batch = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n",
    "    \n",
    "    # Get BERT embeddings for all tokens in the batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens_batch)\n",
    "    \n",
    "    # Extract the [CLS] token embeddings\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    embeddings_list.append(cls_embeddings)\n",
    "\n",
    "# Concatenate embeddings from all batches\n",
    "cls_embeddings = np.concatenate(embeddings_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "926edcc046ce0dd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:59:34.093890Z",
     "start_time": "2024-04-04T06:59:34.091393Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the BERT embeddings to a binarized numpy file\n",
    "with open('../data/bert.npy', 'wb') as f:\n",
    "    np.save(f, cls_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
